Here will briefly write about the seminars and the most important things I learned in them.

---

## 1. Word Embeddings

- **Principial Component Analysis** (PCA) $-$ the simplest linear dimensionality reduction method.

<br>

- **t-distributed Stochastic Neighbor Embedding** (t-SNE) $-$ popular method for exploring high-dimensional data, ability to create compelling two-dimensonal “maps” from data with hundreds or even thousands of dimensions.

<br>

<br>

## 2. Text Classification

- **Generative** vs **Discriminative** $-$ in generative we learn the joint distribution of data (we can then generate this data independently), discriminative - we are only interested in the separating boundary between classes 

<br>

- **Laplace (add-one) smoothing** $-$ adding a small amount to the numerator and denominator of a fraction to yield a non-zero probability of encountering the word in the document 

<br>

- **Naive Bayes classifier** $-$
